{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:15.152528Z",
     "start_time": "2025-10-16T09:45:14.991510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@file:DependsOn(\"ai.koog:koog-agents-jvm:0.5.0\")\n",
    "%use coroutines"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:15.751960Z",
     "start_time": "2025-10-16T09:45:15.731602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.executor.ollama.client.OllamaClient\n",
    "\n",
    "val ollama = OllamaClient()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:23.648412Z",
     "start_time": "2025-10-16T09:45:17.638161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.dsl.prompt\n",
    "import ai.koog.prompt.executor.clients.openai.OpenAIModels\n",
    "import ai.koog.prompt.llm.LLMCapability\n",
    "import ai.koog.prompt.llm.LLMProvider\n",
    "import ai.koog.prompt.llm.LLModel\n",
    "import ai.koog.prompt.llm.OllamaModels\n",
    "import ai.koog.prompt.message.Message\n",
    "\n",
    "val GptOss = LLModel(\n",
    "    provider = LLMProvider.Ollama,\n",
    "    id = \"gpt-oss:20b\",\n",
    "    capabilities = listOf(\n",
    "        LLMCapability.Temperature,\n",
    "        LLMCapability.Schema.JSON.Standard,\n",
    "        LLMCapability.Tools\n",
    "    ),\n",
    "    contextLength = 128_000,\n",
    ")\n",
    "\n",
    "runBlocking {\n",
    "    val responses = ollama.execute(prompt(\"name\") {\n",
    "        system(\"Helpfull assistant!\")\n",
    "        user(\"Tell me a Kotlin Joke?\")\n",
    "    }, GptOss)\n",
    "\n",
    "    responses.forEach { response ->\n",
    "        when (response) {\n",
    "            is Message.Assistant -> println(response.content)\n",
    "            is Message.Tool.Call -> TODO()\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here‚Äôs a quick one:\n",
      "\n",
      "> **Why did the Kotlin developer go broke?**  \n",
      "> Because every time they tried to `return` a value, the compiler kept insisting they **`val`ue** the *immutability* and `lateinit`‚Äëed their savings! üòÑ\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:41.865906Z",
     "start_time": "2025-10-16T09:45:40.635667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.xml.xml\n",
    "import ai.koog.prompt.dsl.PromptBuilder\n",
    "\n",
    "fun PromptBuilder.userQuestion(question: String) {\n",
    "    user {\n",
    "        xml {\n",
    "            tag(\"user-question\") {\n",
    "                +question\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "val system = prompt(\"chat\") {\n",
    "    system {\n",
    "        xml {\n",
    "            tag(\"objective\") { text(\"Helpfull assistant!\") }\n",
    "            tag(\"instructions\") {\n",
    "                tag(\"critical\") {\n",
    "                    +\"Never hallucinate any data, and strictly answer to users questions.\"\n",
    "                    +\"If you cannot answer the question you must say so.\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "val chat1 = prompt(system) {\n",
    "    userQuestion(\"Tell me a Kotlin Joke.\")\n",
    "}\n",
    "\n",
    "val responses = runBlocking {\n",
    "    ollama.execute(chat1, GptOss)\n",
    "}\n",
    "responses"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Assistant(content=Sure, here‚Äôs a quick Kotlin joke for you:\n",
       "\n",
       "Why do Kotlin developers never play hide and seek?\n",
       "\n",
       "Because they always *let* the compiler find them!, metaInfo=ResponseMetaInfo(timestamp=2025-10-16T09:45:41.845882Z, totalTokensCount=209, inputTokensCount=141, outputTokensCount=68, additionalInfo={}, metadata=null), attachments=[], finishReason=null)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:52.046225Z",
     "start_time": "2025-10-16T09:45:50.479634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.text.TextContentBuilder\n",
    "\n",
    "val chat2 = prompt(chat1) {\n",
    "    messages(responses)\n",
    "    userQuestion(\"That was a bad joke! Tell me another better one.\")\n",
    "}\n",
    "\n",
    "val responses2 = runBlocking {\n",
    "    ollama.execute(chat2, GptOss)\n",
    "}\n",
    "responses2"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Assistant(content=I‚Äôm sorry that one didn‚Äôt hit the mark‚Äîlet me try again:\n",
       "\n",
       "Why did the Kotlin program go to therapy?\n",
       "\n",
       "Because it had too many *null* feelings and kept throwing *Exceptions* when it tried to ‚Äúunpack‚Äù its emotions!, metaInfo=ResponseMetaInfo(timestamp=2025-10-16T09:45:52.033873Z, totalTokensCount=305, inputTokensCount=204, outputTokensCount=101, additionalInfo={}, metadata=null), attachments=[], finishReason=null)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:46:14.062714Z",
     "start_time": "2025-10-16T09:46:14.012497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ai.koog.prompt.params.LLMParams\n",
    "\n",
    "val chat3 = prompt(chat2) {\n",
    "    messages(responses2)\n",
    "    user {\n",
    "        text(\"Write a Kotlin function that produces some string.\")\n",
    "    }\n",
    "}\n",
    "chat3.messages.joinToString { it.content }"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<objective>\n",
       "  Helpfull assistant!\n",
       "</objective>\n",
       "<instructions>\n",
       "  <critical>\n",
       "    Never hallucinate any data, and strictly answer to users questions.\n",
       "    If you cannot answer the question you must say so.\n",
       "  </critical>\n",
       "</instructions>, <user-question>\n",
       "  Tell me a Kotlin Joke.\n",
       "</user-question>, Sure, here‚Äôs a quick Kotlin joke for you:\n",
       "\n",
       "Why do Kotlin developers never play hide and seek?\n",
       "\n",
       "Because they always *let* the compiler find them!, <user-question>\n",
       "  That was a bad joke! Tell me another better one.\n",
       "</user-question>, I‚Äôm sorry that one didn‚Äôt hit the mark‚Äîlet me try again:\n",
       "\n",
       "Why did the Kotlin program go to therapy?\n",
       "\n",
       "Because it had too many *null* feelings and kept throwing *Exceptions* when it tried to ‚Äúunpack‚Äù its emotions!, Write a Kotlin function that produces some string."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "2.2.20-Beta2",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  },
  "ktnbPluginMetadata": {
   "projectLibraries": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
